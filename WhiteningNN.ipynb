{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), None and long or byte Variables are valid indices (got numpy.float64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-18f80e433c12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;31m#         transformed_mats_2.append(coord_to_img(np.matmul(R, coord_mat - mean_mat).copy(), bbox=[-60,60,-60,60]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mimg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_and_rotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mtransformed_mats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-18f80e433c12>\u001b[0m in \u001b[0;36mscale_and_rotate\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_coord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mnew_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_new_coord_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0merase_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0mnew_object2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morientation_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minsert_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_object2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-18f80e433c12>\u001b[0m in \u001b[0;36merase_object\u001b[0;34m(img, new_object)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minsert_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_object_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), None and long or byte Variables are valid indices (got numpy.float64)"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from natsort import natsorted\n",
    "import matplotlib.cm as cm\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import mnist\n",
    "cmap = cm.hot\n",
    "\n",
    "def get_img_files(folder):\n",
    "    files = list(filter(lambda file: any(ext in file.lower() for ext in ['png','jpg','bmp']), os.listdir(folder)))\n",
    "    files = natsorted(files)[::1]\n",
    "    images = list(map(lambda x: cv2.imread(folder+'/'+x, 0), files))\n",
    "    return np.array(images)\n",
    "\n",
    "def img_to_coord(img):\n",
    "    coord_mat = np.zeros((3, img.shape[0]*img.shape[1]))\n",
    "    count = 0\n",
    "    for x in range(img.shape[1]):\n",
    "        for y in range(img.shape[0]):\n",
    "            coord_mat[0, count] = x\n",
    "            coord_mat[1, count] = img.shape[0]-y\n",
    "            coord_mat[2, count] = img[y, x]\n",
    "            count += 1\n",
    "    return coord_mat\n",
    "\n",
    "def img_to_coord_tensor(img):\n",
    "    height, width = img.shape\n",
    "    img_transpose = torch.transpose(img, 0, 1).contiguous().view(1, width * height)\n",
    "\n",
    "    x_coord = torch.arange(0, width).view(width, 1)\n",
    "    x_coord = x_coord.expand(width, height).contiguous()\n",
    "    x_coord = x_coord.view(1, width * height)\n",
    "\n",
    "    y_coord = torch.arange(height, 0, -1)\n",
    "    y_coord = y_coord.expand(width, height).contiguous()\n",
    "    y_coord = y_coord.view(1, width * height)\n",
    "\n",
    "    new_coord_matrix = np.vstack([x_coord.numpy(), y_coord.numpy()])\n",
    "    img_tensor = np.vstack([new_coord_matrix, img_transpose.data.numpy()])\n",
    "\n",
    "    return img_tensor\n",
    "\n",
    "def plot_coord_mat(coord_mat, ax):\n",
    "    ax.scatter(coord_mat[0, :], coord_mat[1, :], c=coord_mat[2, :])\n",
    "\n",
    "def rotation_matrix(degrees):\n",
    "    theta = np.radians(degrees)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = np.array(((c,-s, 0), (s, c, 0), (0, 0, 1)))\n",
    "    return R\n",
    "\n",
    "def weighted_pca_regression(x_vec, y_vec, weights):\n",
    "    \"\"\"\n",
    "    Given three real-valued vectors of same length, corresponding to the coordinates\n",
    "    and weight of a 2-dimensional dataset, this function outputs the angle in radians\n",
    "    of the line that aligns with the (weighted) average and main linear component of\n",
    "    the data. For that, first a weighted mean and covariance matrix are computed.\n",
    "    Then u,e,v=svd(cov) is performed, and u * f(x)=0 is solved.\n",
    "    \"\"\"\n",
    "    input_mat = np.stack([x_vec, y_vec])\n",
    "    weights_sum = weights.sum()\n",
    "    # Subtract (weighted) mean and compute (weighted) covariance matrix:\n",
    "    mean_x, mean_y =  weights.dot(x_vec)/weights_sum, weights.dot(y_vec)/weights_sum\n",
    "    centered_x, centered_y = x_vec-mean_x, y_vec-mean_y\n",
    "    matrix_centered = np.stack([centered_x, centered_y])\n",
    "    weighted_cov = matrix_centered.dot(np.diag(weights).dot(matrix_centered.T)) / weights_sum\n",
    "    # We know that v rotates the data's main component onto the y=0 axis, and\n",
    "    # that u rotates it back. Solving u.dot([x,0])=[x*u[0,0], x*u[1,0]] gives\n",
    "    # f(x)=(u[1,0]/u[0,0])x as the reconstructed function.\n",
    "    u,e,v = np.linalg.svd(weighted_cov)\n",
    "    eig_val = np.sqrt(e)\n",
    "    if u[1,0] < 0:\n",
    "        eig_val[0] = -1*eig_val[0]\n",
    "    if u[0,1] < 0:\n",
    "        eig_val[1] = -1*eig_val[1]\n",
    "        \n",
    "    return eig_val[0]*u[:,0], eig_val[1]*u[:,1], np.array([mean_x, mean_y])\n",
    "        \n",
    "def coord_to_img(coord_mat, bbox=[-60,60,-60,60]):\n",
    "    x_min = bbox[0]\n",
    "    x_max = bbox[1]\n",
    "    y_min = bbox[2]\n",
    "    y_max = bbox[3]\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "    \n",
    "    img = np.zeros([y_max-y_min, x_max-x_min])\n",
    "    for col in range(coord_mat.shape[1]):\n",
    "        x = coord_mat[0, col]\n",
    "        y = coord_mat[1, col]\n",
    "        val = coord_mat[2, col]\n",
    "        new_x = int(x - x_min)\n",
    "        new_y = int(y_max - y)\n",
    "        if (0 <= new_x) and (new_x < width) and (0 <= new_y) and (new_y < height):\n",
    "            img[new_y, new_x] = val\n",
    "    return img\n",
    "\n",
    "def object_extraction(coord_mat, min_val=0.2):\n",
    "    min_pixel_val = np.repeat([[min_val]], coord_mat.shape[1], axis=1) \n",
    "    extracted_object_coords = np.greater(coord_mat, min_pixel_val)\n",
    "    new_object = coord_mat.copy()[:,extracted_object_coords[2,:]]\n",
    "    coord_mat[2,extracted_object_coords[2,:]] = 0\n",
    "    return new_object\n",
    "\n",
    "def orientation_normalization(coord_mat):\n",
    "        weights = coord_mat[2, :]\n",
    "        primary_vector, seconday_vector, mean_vector = weighted_pca_regression(coord_mat[0,:], coord_mat[1,:], coord_mat[2,:])\n",
    "        angle = np.arctan2(primary_vector[1], primary_vector[0]) * 180 / np.pi\n",
    "        R = rotation_matrix(90-angle)\n",
    "        mean_mat = np.array([mean_vector[0], mean_vector[1], 0])\n",
    "        mean_mat = np.diag(mean_mat)\n",
    "        mean_mat = np.matmul(mean_mat, np.ones(coord_mat.shape))\n",
    "        eig_1 = np.linalg.norm(primary_vector)\n",
    "        eig_2 = np.linalg.norm(seconday_vector)\n",
    "        norm_scale_mat = np.zeros([3,3])\n",
    "        norm_scale_mat[0,0] = 9/eig_2\n",
    "        norm_scale_mat[1,1] = 18/eig_1\n",
    "        norm_scale_mat[2,2] = 1\n",
    "        transformed_mat = np.matmul(norm_scale_mat, np.matmul(R, coord_mat - mean_mat))\n",
    "        return transformed_mat, R, mean_mat\n",
    "    \n",
    "def object_extraction(coord_mat, min_val=0.2):\n",
    "    min_pixel_val = np.repeat([[min_val]], coord_mat.shape[1], axis=1) \n",
    "    extracted_object_coords = np.greater(coord_mat, min_pixel_val)\n",
    "    new_object = coord_mat.copy()[:,extracted_object_coords[2,:]]\n",
    "    coord_mat[2,extracted_object_coords[2,:]] = 0\n",
    "    return new_object\n",
    "\n",
    "def orientation_normalization(coord_mat):\n",
    "        weights = coord_mat[2, :]\n",
    "        primary_vector, seconday_vector, mean_vector = weighted_pca_regression(coord_mat[0,:], coord_mat[1,:], coord_mat[2,:])\n",
    "        angle = np.arctan2(primary_vector[1], primary_vector[0]) * 180 / np.pi\n",
    "        R = rotation_matrix(90-angle)\n",
    "        mean_mat = np.array([mean_vector[0], mean_vector[1], 0])\n",
    "        mean_mat = np.diag(mean_mat)\n",
    "        mean_mat = np.matmul(mean_mat, np.ones(coord_mat.shape))\n",
    "        eig_1 = np.linalg.norm(primary_vector)\n",
    "        eig_2 = np.linalg.norm(seconday_vector)\n",
    "        norm_scale_mat = np.zeros([3,3])\n",
    "        norm_scale_mat[0,0] = 9/eig_2\n",
    "        norm_scale_mat[1,1] = 18/eig_1\n",
    "        norm_scale_mat[2,2] = 1\n",
    "        transformed_mat = np.matmul(norm_scale_mat, np.matmul(R, coord_mat - mean_mat))\n",
    "        return transformed_mat, R, mean_mat\n",
    "\n",
    "def erase_object(img, new_object):\n",
    "    for col_num in range(new_object.shape[1]):\n",
    "        col = new_object[:,col_num]\n",
    "        img[img.shape[0]-col[1], col[0]] = 0\n",
    "\n",
    "def insert_object(img, new_object_coords, location):\n",
    "    for col_num in range(new_object_coords.shape[1]):\n",
    "        col = new_object_coords[:, col_num]\n",
    "        x = np.clip(location[0] + col[0], 0, img.shape[1] - 1)\n",
    "        y = np.clip(-1*(location[1] + col[1] - 1)+img.shape[0], 0, img.shape[0] - 1)\n",
    "        spill_over_x = x % 1\n",
    "        spill_over_y = y % 1\n",
    "        ceil_x = int(np.ceil(x))\n",
    "        lower_x_weight = 1 - spill_over_x\n",
    "        heigher_x_weight = spill_over_x\n",
    "        \n",
    "        ceil_y = int(np.ceil(y))\n",
    "        lower_y_weight = 1 - spill_over_y\n",
    "        heigher_y_weight = spill_over_y   \n",
    "        \n",
    "        pixel_value = col[2]\n",
    "        max_pixel_val = 0.5\n",
    "        img[ceil_y, ceil_x] = img[ceil_y, ceil_x] + pixel_value*heigher_y_weight*heigher_x_weight\n",
    "        img[ceil_y, ceil_x - 1] = img[ceil_y, ceil_x - 1] + pixel_value*lower_x_weight*heigher_y_weight\n",
    "        img[ceil_y - 1, ceil_x] = img[ceil_y - 1, ceil_x] + pixel_value*lower_y_weight*heigher_x_weight\n",
    "        img[ceil_y - 1, ceil_x - 1] = img[ceil_y - 1, ceil_x - 1] + pixel_value*lower_y_weight*lower_x_weight\n",
    "        img[ceil_y, ceil_x] = min(float(img[ceil_y, ceil_x]), max_pixel_val)\n",
    "        img[ceil_y, ceil_x - 1] = min(float(img[ceil_y, ceil_x - 1]), max_pixel_val)\n",
    "        img[ceil_y - 1, ceil_x] = min(float(img[ceil_y - 1, ceil_x]), max_pixel_val)\n",
    "        img[ceil_y - 1, ceil_x - 1] = min(float(img[ceil_y - 1, ceil_x - 1]), max_pixel_val)\n",
    "#         img[ceil_y, ceil_x] = pixel_value\n",
    "    return img\n",
    "\n",
    "def scale_and_rotate(img):\n",
    "    new_new_coord_mat = img_to_coord_tensor(img)\n",
    "    test = img_to_coord(img)\n",
    "    new_object = object_extraction(new_new_coord_mat, min_val=0)\n",
    "    erase_object(img, new_object)\n",
    "    new_object2, _, _ = orientation_normalization(new_object)\n",
    "    img = insert_object(img, new_object2, [50, 50])\n",
    "    return img\n",
    "    \n",
    "\n",
    "\n",
    "for folder in ['fives','sixes','sevens','eights','nines','people'][:]:\n",
    "    imgs = get_img_files(os.getcwd()+'/'+folder)\n",
    "\n",
    "    coord_mats = []\n",
    "    transformed_mats = []\n",
    "    transformed_mats_2 = []\n",
    "#     fig, ax = plt.subplots(3, figsize=(10,10))\n",
    "    for index, img in enumerate(imgs[:]):\n",
    "        img = cv2.resize(img, (100,100))\n",
    "        img = img/img.max()\n",
    "        img2 = Variable(torch.from_numpy(img))\n",
    "        coord_mat = img_to_coord(img)\n",
    "        coord_mat_2 = img_to_coord_tensor(img2)\n",
    "#          plot_coord_mat(, ax[1])\n",
    "#         ax[0].set_xlim([0,100])\n",
    "#         ax[0].set_ylim([0,100])\n",
    "#         ax[1].set_xlim([-60,60])\n",
    "#         ax[1].set_ylim([-60,60])\n",
    "#         ax[1].set_title(\"{}, {}\".format(eig_1, eig_2))\n",
    "#         tansform_mat, R, mean_mat = orientation_normalization(coord_mat.copy())\n",
    "        coord_mats.append(coord_to_img(coord_mat.copy(), bbox=[0,100,0,100]))\n",
    "#         transformed_mats_2.append(coord_to_img(np.matmul(R, coord_mat - mean_mat).copy(), bbox=[-60,60,-60,60]))\n",
    "        \n",
    "        img2 = scale_and_rotate(img2)\n",
    "        print(folder, index)\n",
    "        transformed_mats.append(img2.data.numpy())\n",
    "        del img2\n",
    "\n",
    "    fig2, ax2 = plt.subplots(1,2,figsize=(15,15))\n",
    "    combined_orig = []\n",
    "    combined_transformed = []\n",
    "    # plot_coord_mat(coord_mats[0] + coord_mats[1], ax2[0])\n",
    "    combined_orig = sum(coord_mats)\n",
    "    combined_transformed = sum(transformed_mats)\n",
    "#     combined_transformed_2 = sum(transformed_mats_2)\n",
    "    ax2[0].imshow(combined_orig)\n",
    "    ax2[0].set_title(\"Overlay of Original\")\n",
    "#     ax2[1].imshow(combined_transformed_2)\n",
    "#     ax2[1].set_title(\"Overlay of Rotated\")\n",
    "    ax2[1].imshow(combined_transformed)\n",
    "    ax2[1].set_title(\"Overlay of Rotated & scaled\")\n",
    "#     fig2.savefig(\"Sevens_full.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [7],\n",
       "       ...,\n",
       "       [3],\n",
       "       [7],\n",
       "       [1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "mat_contents = sio.loadmat('/mnt/c/Users/Rees/Downloads/training_and_validation_batches/training_and_validation_batches/1.mat')\n",
    "nums = mat_contents['affNISTdata']\n",
    "import matplotlib.pyplot as plt\n",
    "print(nums[0][0][5].shape)\n",
    "labels = nums[0][0][5].T\n",
    "imgs = np.transpose(nums[0][0][2].reshape((40,40, 60000)), (2,0,1))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size:  torch.Size([60000, 28, 28]) - torch.Size([60000])\n",
      "Testing Data Size:  torch.Size([10000, 28, 28]) - torch.Size([10000])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import datasets\n",
    "import torch as th\n",
    "# Change this to where you want to save the data\n",
    "SAVE_DIR = os.path.expanduser(os.getcwd()+'/data/MNIST/')\n",
    "# train data\n",
    "mnist_train = datasets.MNIST(SAVE_DIR, train=True, download=False)\n",
    "x_train_mnist, y_train_mnist = mnist_train.train_data.type(th.FloatTensor), mnist_train.train_labels\n",
    "# test data\n",
    "mnist_test = datasets.MNIST(SAVE_DIR, train=False, download=True)\n",
    "x_test_mnist, y_test_mnist = mnist_test.test_data.type(th.FloatTensor), mnist_test.test_labels\n",
    "\n",
    "print('Training Data Size: ' ,x_train_mnist.size(), '-', y_train_mnist.size())\n",
    "print('Testing Data Size: ' ,x_test_mnist.size(), '-', y_test_mnist.size())\n",
    "\n",
    "plt.imshow(x_train_mnist[0].numpy(), cmap='gray')\n",
    "plt.title('DIGIT: %i' % y_train_mnist[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsample.transforms import RandomRotate\n",
    "from torchsample.transforms  import AddChannel\n",
    "\n",
    "from  torchsample.transforms import RandomCrop\n",
    "# note that we DONT add the channel dim to transform - the same crop will be applied to each channel\n",
    "rand_crop = RandomCrop((20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 4\n",
    "x_example = add_channel(x_train_mnist[index])\n",
    "plt.imshow(x_example[0].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0.])\n",
      "tensor([[[ 0.0140,  0.9999,  0.0000],\n",
      "         [ 0.9999, -0.0140,  0.0000]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "rotation = torch.Tensor([[1/1.0,0],[0, 1/1.0]])\n",
    "rotation = rot\n",
    "translation = torch.Tensor([0, 0])\n",
    "theta = torch.Tensor([[0, 0, 0],[0, 0, 0]])\n",
    "theta[:2, :2] = rotation\n",
    "print(theta[:, 2])\n",
    "theta[:, 2] = translation\n",
    "theta = theta.view(1, 2, 3)\n",
    "print(theta)\n",
    "x_batch = x_example.view(1, 1, 28, 28)\n",
    "x_flat = x_example.view(1, 28 * 28)\n",
    "grid = F.affine_grid(theta, x_batch.size())\n",
    "x = F.grid_sample(x_batch, grid)\n",
    "plt.imshow(x[0][0])\n",
    "\n",
    "u, s, v = torch.svd(x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n",
      "U: tensor([[ 0.0140,  0.9999],\n",
      "        [ 0.9999, -0.0140]])\n",
      "tensor([[ 0.0140,  0.9999],\n",
      "        [ 0.9999, -0.0140]]) tensor(13.7118) tensor(13.9205)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1d0c546be0>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def img_to_coord_tensor(img, img_num, tensor_index):\n",
    "    \"\"\"\n",
    "    Takes in a 4D image and returns the 3*W*H coordinate representation\n",
    "    \"\"\"\n",
    "    _, _, height, width = img.shape\n",
    "    flattened_img = torch.transpose(img[img_num, tensor_index], 0, 1).contiguous().view(width * height)\n",
    "    x_coord = torch.arange(0, width).view(width, 1)\n",
    "    x_coord = x_coord.expand(width, height).contiguous()\n",
    "    x_coord = x_coord.view(width * height).float()\n",
    "\n",
    "    y_coord = torch.arange(height, 0, -1)\n",
    "    y_coord = y_coord.expand(width, height).contiguous()\n",
    "    y_coord = y_coord.view(width * height).float()\n",
    "\n",
    "    new_coord_matrix = np.vstack([x_coord.numpy(), y_coord.numpy()])\n",
    "    img_tensor = torch.stack((x_coord, y_coord, flattened_img))\n",
    "\n",
    "    return img_tensor\n",
    "\n",
    "def weighted_pca(coord_mat):\n",
    "    \"\"\"\n",
    "    Given three real-valued vectors of same length, corresponding to the coordinates\n",
    "    and weight of a 2-dimensional dataset, this function outputs the angle in radians\n",
    "    of the line that aligns with the (weighted) average and main linear component of\n",
    "    the data. For that, first a weighted mean and covariance matrix are computed.\n",
    "    Then u,e,v=svd(cov) is performed, and u * f(x)=0 is solved.\n",
    "    \"\"\"\n",
    "    x_vec = coord_mat[0, :]\n",
    "    y_vec = coord_mat[1, :]\n",
    "    weights = coord_mat[2, :]\n",
    "    weight_mat = torch.diag(weights)\n",
    "    weights_sum = weights.sum()\n",
    "    # Subtract (weighted) mean and compute (weighted) covariance matrix:\n",
    "    mean_x, mean_y = torch.dot(weights, x_vec)/weights_sum, torch.dot(weights, y_vec)/weights_sum\n",
    "    coord_mat[0, :] -= mean_x\n",
    "    coord_mat[1, :] -= mean_y\n",
    "    centered_mat = coord_mat[[0, 1], :]\n",
    "    weighted_cov = torch.mm(centered_mat, torch.mm(weight_mat, torch.t(centered_mat) )) / weights_sum\n",
    "    u,e,v = torch.svd(weighted_cov)\n",
    "    eig_val = torch.sqrt(e)\n",
    "    if u[1,0] < 0:\n",
    "        eig_val[0] = -1*eig_val[0]\n",
    "    if u[0,1] < 0:\n",
    "        eig_val[1] = -1*eig_val[1]\n",
    "    print(\"U:\", u)\n",
    "    scale_mat = torch.Tensor([[]])\n",
    "#     rotation_mat = torch.stack([eig_val[0]**(-1)*u[:,0], eig_val[1]**(-1)*u[:,1]])\n",
    "#     rotation_mat = torch.t(rotation_mat)\n",
    "    return u, mean_x, mean_y\n",
    "\n",
    "    \n",
    "# def frame_decomposition(img_batch, img_num, tensor_index):\n",
    "#     coord_mat = img_to_coord_tensor(img_batch, img_num, tensor_index)\n",
    "#     weights = coord_mat[2, :] \n",
    "\n",
    "# def orientation_normalization(self, coord_mat, eig_width, eig_height):\n",
    "#     weights = coord_mat[2, :]\n",
    "#     primary_vector, seconday_vector, mean_vector = self.weighted_pca_regression(coord_mat[0,:], coord_mat[1,:], coord_mat[2,:])\n",
    "#     angle = np.arctan2(primary_vector[1], primary_vector[0]) * 180 / np.pi\n",
    "#     R = rotation_matrix(90-angle)\n",
    "#     mean_mat = np.array([mean_vector[0], mean_vector[1], 0])\n",
    "#     mean_mat = np.diag(mean_mat)\n",
    "#     mean_mat = np.matmul(mean_mat, np.ones(coord_mat.shape))\n",
    "#     eig_1 = np.linalg.norm(primary_vector)\n",
    "#     eig_2 = np.linalg.norm(seconday_vector)\n",
    "#     norm_scale_mat = np.zeros([3,3])\n",
    "#     norm_scale_mat[0,0] = 1\n",
    "#     norm_scale_mat[1,1] = 1\n",
    "#     norm_scale_mat[2,2] = 1\n",
    "#     return norm\n",
    "#     transformed_mat = np.matmul(norm_scale_mat, np.matmul(R, coord_mat - mean_mat))\n",
    "#     return transformed_mat, (90-angle), mean_mat\n",
    "\n",
    "x_batch_coord = img_to_coord_tensor(x_batch, 0, 0)\n",
    "print(x_batch_coord.shape)\n",
    "rot, mean_x, mean_y = weighted_pca(x_batch_coord)\n",
    "print(rot, mean_x, mean_y)\n",
    "plt.imshow(x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [2., 2.]])\n",
      "tensor([[ 1.,  1.],\n",
      "        [-3., -3.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[1,1], [2,2]])\n",
    "print(a)\n",
    "b = a\n",
    "b[1, :] -= 5\n",
    "print(b[[0,1], :])\n",
    "# torch.mm(a, b, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1,2,3,4]).long()\n",
    "b = torch.Tensor([5,6,7,8]).long()\n",
    "torch.stack((a,b, b)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_36_env",
   "language": "python",
   "name": "py_36_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
